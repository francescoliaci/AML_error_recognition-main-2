{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a882e3f"
   },
   "source": [
    "# **AML Mistake Recognition - LSTM Baseline Training**\n",
    "\n",
    "This notebook trains and evaluates the new LSTM baseline for mistake detection in procedural activities.\n",
    "\n",
    "**What's new:**\n",
    "- Train LSTM baseline from scratch\n",
    "- Compare with MLP and Transformer baselines\n",
    "- Evaluate on step and recordings splits\n",
    "- Generate comparison visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIlgnpQqCFUB"
   },
   "source": [
    "# **1 Initial Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hH5GFggQCJXX"
   },
   "source": [
    "## 1.1 Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lx6FtK902Vv2"
   },
   "outputs": [],
   "source": [
    "!pip install torcheval wandb -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6qxXj_ZCSBj"
   },
   "source": [
    "## 1.2 Import the project (with LSTM baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nPZy-COgahVV"
   },
   "outputs": [],
   "source": [
    "## Delete /code if it already exists and you need to reclone the project\n",
    "!rm -rf code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UiUFeWTG1jnB"
   },
   "outputs": [],
   "source": [
    "# Clone the project repository\n",
    "!git clone --recursive https://github.com/SimoneColu/AML_error_recognition.git code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Add LSTM Implementation Files\n",
    "\n",
    "**Option 1:** If your repository already has the LSTM files, skip this cell.\n",
    "\n",
    "**Option 2:** If not, you need to upload the LSTM files we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLY RUN THIS IF YOUR REPO DOESN'T HAVE THE LSTM FILES YET\n",
    "\n",
    "# You have two options:\n",
    "# Option A: Upload from your local machine (click the folder icon, then upload)\n",
    "# Upload these files to /content/:\n",
    "#   - er_lstm.py (goes to code/core/models/)\n",
    "#   - updated base.py (goes to code/)\n",
    "#   - updated constants.py (goes to code/)\n",
    "\n",
    "# Option B: Copy from your Google Drive (if you've already uploaded them there)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Copy files from Drive to the project\n",
    "# !cp /content/drive/MyDrive/AML_DAAI_25_26/er_lstm.py code/core/models/\n",
    "# !cp /content/drive/MyDrive/AML_DAAI_25_26/base.py code/\n",
    "# !cp /content/drive/MyDrive/AML_DAAI_25_26/constants.py code/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Verify LSTM Files Are Present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that LSTM model file exists\n",
    "!ls -lh code/core/models/er_lstm.py\n",
    "\n",
    "# Check that constants have LSTM variants\n",
    "!grep \"LSTM_VARIANT\" code/constants.py\n",
    "\n",
    "# Check that base.py supports LSTM\n",
    "!grep -A 3 \"LSTM_VARIANT\" code/base.py | head -10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwxM3lpPCWeX"
   },
   "source": [
    "## 1.3 Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NXArnjm51vZx"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQluB3_UCjwA"
   },
   "source": [
    "## 1.4 Set the constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPcoTvuHDWG-"
   },
   "outputs": [],
   "source": [
    "DRIVE_PATH = \"/content/drive/MyDrive/AML_DAAI_25_26\"\n",
    "LOCAL_CODE_PATH = \"/content/code\"\n",
    "\n",
    "## Create necessary directories\n",
    "!mkdir -p {LOCAL_CODE_PATH}/data/video\n",
    "!mkdir -p {LOCAL_CODE_PATH}/checkpoints/error_recognition/LSTM/omnivore\n",
    "!mkdir -p {LOCAL_CODE_PATH}/checkpoints/error_recognition/LSTM/slowfast\n",
    "!mkdir -p {LOCAL_CODE_PATH}/checkpoints/error_recognition/GRU/omnivore\n",
    "!mkdir -p {LOCAL_CODE_PATH}/checkpoints/error_recognition/LSTM_Attention/omnivore\n",
    "!mkdir -p {LOCAL_CODE_PATH}/results\n",
    "!mkdir -p {LOCAL_CODE_PATH}/plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fy9QJ0NGECnF"
   },
   "source": [
    "# **2 Extract Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6eW6R_aGieg"
   },
   "source": [
    "## 2.1 Extract Omnivore Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrXZWd_OIHQP"
   },
   "outputs": [],
   "source": [
    "# Extract Omnivore features (quiet mode)\n",
    "!unzip -q \"{DRIVE_PATH}/data/backbone/omnivore.zip\" -d {LOCAL_CODE_PATH}/data/video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Extract SlowFast Features (Optional)\n",
    "\n",
    "Uncomment if you want to train on SlowFast features as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip -q \"{DRIVE_PATH}/data/backbone/slowfast.zip\" -d {LOCAL_CODE_PATH}/data/video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3 Train LSTM Baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Train LSTM on Step Split (Omnivore)\n",
    "\n",
    "This will train the LSTM model on the step split. Expected training time: **1-2 hours**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd code\n",
    "python train_er.py \\\n",
    "  --variant LSTM \\\n",
    "  --backbone omnivore \\\n",
    "  --split step \\\n",
    "  --num_epochs 50 \\\n",
    "  --lr 1e-3 \\\n",
    "  --batch_size 1 \\\n",
    "  --weight_decay 1e-3 \\\n",
    "  --seed 42 \\\n",
    "  --ckpt_directory /content/code/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Train LSTM on Recordings Split (Omnivore)\n",
    "\n",
    "This will train on the more challenging recordings split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd code\n",
    "python train_er.py \\\n",
    "  --variant LSTM \\\n",
    "  --backbone omnivore \\\n",
    "  --split recordings \\\n",
    "  --num_epochs 50 \\\n",
    "  --lr 1e-3 \\\n",
    "  --batch_size 1 \\\n",
    "  --weight_decay 1e-3 \\\n",
    "  --seed 42 \\\n",
    "  --ckpt_directory /content/code/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Train Alternative Variants (Optional)\n",
    "\n",
    "### 3.3.1 LSTM with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd code\n",
    "python train_er.py \\\n",
    "  --variant LSTM_Attention \\\n",
    "  --backbone omnivore \\\n",
    "  --split step \\\n",
    "  --num_epochs 50 \\\n",
    "  --lr 1e-3 \\\n",
    "  --batch_size 1 \\\n",
    "  --weight_decay 1e-3 \\\n",
    "  --seed 42 \\\n",
    "  --ckpt_directory /content/code/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 GRU Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd code\n",
    "python train_er.py \\\n",
    "  --variant GRU \\\n",
    "  --backbone omnivore \\\n",
    "  --split step \\\n",
    "  --num_epochs 50 \\\n",
    "  --lr 1e-3 \\\n",
    "  --batch_size 1 \\\n",
    "  --weight_decay 1e-3 \\\n",
    "  --seed 42 \\\n",
    "  --ckpt_directory /content/code/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Monitor Training Progress\n",
    "\n",
    "Check the training logs to see how the model is performing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View training statistics\n",
    "!tail -20 code/stats/error_recognition/LSTM/omnivore/*_training_performance.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4 Evaluate LSTM Baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Evaluate LSTM - Step Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd code\n",
    "python -m core.evaluate \\\n",
    "  --variant LSTM \\\n",
    "  --backbone omnivore \\\n",
    "  --ckpt checkpoints/error_recognition/LSTM/omnivore/error_recognition_step_omnivore_LSTM_video_best.pt \\\n",
    "  --split step \\\n",
    "  --threshold 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Evaluate LSTM - Recordings Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd code\n",
    "python -m core.evaluate \\\n",
    "  --variant LSTM \\\n",
    "  --backbone omnivore \\\n",
    "  --ckpt checkpoints/error_recognition/LSTM/omnivore/error_recognition_recordings_omnivore_LSTM_video_best.pt \\\n",
    "  --split recordings \\\n",
    "  --threshold 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5 Compare with Existing Baselines**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Evaluate MLP Baseline (if checkpoint available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy MLP checkpoint from Drive\n",
    "!cp \"{DRIVE_PATH}/data/checkpoint/MLP/error_recognition_MLP_omnivore_step_epoch_43.pt\" code/checkpoints/error_recognition_best/MLP/omnivore/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd code\n",
    "python -m core.evaluate \\\n",
    "  --variant MLP \\\n",
    "  --backbone omnivore \\\n",
    "  --ckpt checkpoints/error_recognition_best/MLP/omnivore/error_recognition_MLP_omnivore_step_epoch_43.pt \\\n",
    "  --split step \\\n",
    "  --threshold 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Evaluate Transformer Baseline (if checkpoint available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Transformer checkpoint from Drive\n",
    "!cp \"{DRIVE_PATH}/data/checkpoint/Transformer/error_recognition_Transformer_omnivore_step_epoch_9.pt\" code/checkpoints/error_recognition_best/Transformer/omnivore/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd code\n",
    "python -m core.evaluate \\\n",
    "  --variant Transformer \\\n",
    "  --backbone omnivore \\\n",
    "  --ckpt checkpoints/error_recognition_best/Transformer/omnivore/error_recognition_Transformer_omnivore_step_epoch_9.pt \\\n",
    "  --split step \\\n",
    "  --threshold 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6 Visualize Results**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 View Results CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read results for step split\n",
    "df = pd.read_csv('code/results/error_recognition/combined_results/step_True_substep_True_threshold_0.6.csv')\n",
    "print(\"\\n===== STEP SPLIT RESULTS =====\")\n",
    "print(df)\n",
    "\n",
    "# Filter for main variants\n",
    "print(\"\\n===== COMPARISON (MLP vs Transformer vs LSTM) =====\")\n",
    "comparison = df[df['Variant'].isin(['MLP', 'Transformer', 'LSTM'])]\n",
    "print(comparison[['Variant', 'Backbone', 'Step F1', 'Step AUC', 'Step Precision', 'Step Recall']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Generate Comparison Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "# Read results\n",
    "df = pd.read_csv('code/results/error_recognition/combined_results/step_True_substep_True_threshold_0.6.csv')\n",
    "main_variants = df[df['Variant'].isin(['MLP', 'Transformer', 'LSTM'])]\n",
    "\n",
    "# Plot F1 comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(data=main_variants, x='Variant', y='Step F1', palette='Set2')\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(ax.patches):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.2f}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.title('F1 Score Comparison (Step Split)', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('F1 Score', fontsize=12)\n",
    "plt.xlabel('Model Variant', fontsize=12)\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('code/plots/f1_comparison_step.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to: code/plots/f1_comparison_step.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Plot All Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all metrics\n",
    "metrics = ['Step F1', 'Step AUC', 'Step Precision', 'Step Recall']\n",
    "plot_data = main_variants[['Variant'] + metrics].copy()\n",
    "plot_data_melted = plot_data.melt(id_vars='Variant', var_name='Metric', value_name='Score')\n",
    "plot_data_melted['Metric'] = plot_data_melted['Metric'].str.replace('Step ', '')\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "ax = sns.barplot(data=plot_data_melted, x='Metric', y='Score', hue='Variant', palette='Set2')\n",
    "plt.title('All Metrics Comparison (Step Split)', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Score', fontsize=12)\n",
    "plt.xlabel('Metric', fontsize=12)\n",
    "plt.legend(title='Model', fontsize=11)\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('code/plots/all_metrics_comparison.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPlot saved to: code/plots/all_metrics_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Learning Curves (if training logs available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Read training log\n",
    "try:\n",
    "    log_file = !ls code/stats/error_recognition/LSTM/omnivore/*_training_performance.txt\n",
    "    log_file = log_file[0] if log_file else None\n",
    "    \n",
    "    if log_file:\n",
    "        # Parse log file\n",
    "        data = np.genfromtxt(log_file, delimiter=',', skip_header=1)\n",
    "        epochs = data[:, 0]\n",
    "        train_loss = data[:, 1]\n",
    "        val_loss = data[:, 2]\n",
    "        f1 = data[:, 6]\n",
    "        auc = data[:, 7]\n",
    "        \n",
    "        # Plot learning curves\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Loss curves\n",
    "        ax1.plot(epochs, train_loss, label='Train Loss', linewidth=2, color='#3498db')\n",
    "        ax1.plot(epochs, val_loss, label='Val Loss', linewidth=2, color='#e74c3c')\n",
    "        ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "        ax1.set_xlabel('Epoch', fontsize=12)\n",
    "        ax1.set_ylabel('Loss', fontsize=12)\n",
    "        ax1.legend(fontsize=11)\n",
    "        ax1.grid(alpha=0.3)\n",
    "        \n",
    "        # Metrics curves\n",
    "        ax2.plot(epochs, f1, label='F1 Score', linewidth=2, color='#2ecc71')\n",
    "        ax2.plot(epochs, auc, label='AUC', linewidth=2, color='#f39c12')\n",
    "        ax2.set_title('Validation Metrics', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Epoch', fontsize=12)\n",
    "        ax2.set_ylabel('Score', fontsize=12)\n",
    "        ax2.legend(fontsize=11)\n",
    "        ax2.grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('code/plots/lstm_learning_curves.png', bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nPlot saved to: code/plots/lstm_learning_curves.png\")\n",
    "    else:\n",
    "        print(\"Training log not found. Train the model first.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error plotting learning curves: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7 Save Results to Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory in Drive\n",
    "!mkdir -p \"{DRIVE_PATH}/results/lstm_baseline\"\n",
    "\n",
    "# Copy checkpoints\n",
    "!cp -r code/checkpoints/error_recognition/LSTM \"{DRIVE_PATH}/results/lstm_baseline/checkpoints/\"\n",
    "\n",
    "# Copy results\n",
    "!cp -r code/results/error_recognition \"{DRIVE_PATH}/results/lstm_baseline/results/\"\n",
    "\n",
    "# Copy plots\n",
    "!cp -r code/plots \"{DRIVE_PATH}/results/lstm_baseline/plots/\"\n",
    "\n",
    "# Copy training logs\n",
    "!cp -r code/stats/error_recognition/LSTM \"{DRIVE_PATH}/results/lstm_baseline/training_logs/\"\n",
    "\n",
    "print(\"\\nAll results saved to Google Drive!\")\n",
    "print(f\"Location: {DRIVE_PATH}/results/lstm_baseline/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8 Summary and Next Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Print Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"LSTM BASELINE TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nWhat you've accomplished:\")\n",
    "print(\"âœ… Trained LSTM baseline on step and/or recordings split\")\n",
    "print(\"âœ… Evaluated LSTM performance with detailed metrics\")\n",
    "print(\"âœ… Compared with MLP and Transformer baselines\")\n",
    "print(\"âœ… Generated visualization plots\")\n",
    "print(\"âœ… Saved all results to Google Drive\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Analyze the per-error-type performance (check evaluation output)\")\n",
    "print(\"2. Conduct ablation studies (single-layer, unidirectional, etc.)\")\n",
    "print(\"3. Try LSTM_Attention and GRU variants\")\n",
    "print(\"4. Write your project report using the results\")\n",
    "\n",
    "print(\"\\nDocumentation:\")\n",
    "print(\"ðŸ“„ NEW_BASELINE_PROPOSAL.md - Detailed motivation and architecture\")\n",
    "print(\"ðŸ“„ BASELINE_COMPARISON_ANALYSIS.md - Comprehensive comparison\")\n",
    "print(\"ðŸ“„ LSTM_QUICKSTART_GUIDE.md - Training and evaluation guide\")\n",
    "print(\"ðŸ“„ SUMMARY.md - High-level overview\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

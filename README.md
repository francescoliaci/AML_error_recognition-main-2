# LSTM Baseline for Mistake Detection in Procedural Activities

This project implements a new baselines for supervised error recognition in cooking videos: an LSTM that processes sequences bidirectionally

## Quick Start

The easiest way to get started is using the Colab notebook:

ðŸ““ **[colab_lstm_training.ipynb](colab_lstm_training.ipynb)**

The notebook handles environment setup, feature extraction, training, and evaluation across all three models

## Architecture

The LSTM processes variable-length sequences of video features through two bidirectional LSTM layers, followed by a classification head. The model uses dropout for regularization and outputs a binary prediction (correct vs. error).

```
Input (T Ã— 1024) â†’ BiLSTM(512) â†’ Dropout â†’ BiLSTM(256) â†’ Dropout
  â†’ Linear(512, 256) â†’ ReLU â†’ Dropout â†’ Linear(256, 1) â†’ Output
```

## Project Structure

```
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ er_lstm.py          # LSTM implementation
â”‚   â”‚   â”œâ”€â”€ er_former.py        # Transformer baseline
â”‚   â”‚   â””â”€â”€ blocks.py           # MLP baseline
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ config.py
â”œâ”€â”€ dataloader/
â”‚   â”œâ”€â”€ CaptainCookStepDataset.py
â”‚   â””â”€â”€ CaptainCookSubStepDataset.py
â”œâ”€â”€ train_er.py
â”œâ”€â”€ colab_lstm_training.ipynb
â””â”€â”€ README.md
```

## Configuration

```python
learning_rate = 1e-3
batch_size = 1
optimizer = Adam(weight_decay=1e-3)
loss = BCEWithLogitsLoss(pos_weight=2.5)
epochs = 50
```

---

**Last updated**: January 2026
